\documentclass[b5paper,10pt,twoside]{article}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[b5paper,centering]{geometry}
\usepackage{fancyhdr}

\usepackage{mathtools}
\usepackage{amsmath}


\setlength\textheight{184mm}
\setlength\textwidth{127mm}

\pagestyle{fancy}
\renewcommand{\leftmark}{P. Janus, T. Kryjak, M. Gorgoń}
\renewcommand{\rightmark}{Tutaj tytuł}
\fancyhead{}
\fancyhead[RE]{\leftmark}
\fancyhead[LE]{\thepage}
\fancyhead[LO]{\rightmark}
\fancyhead[RO]{\thepage}
\fancyfoot{}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

%\usepackage{titlesec}
\usepackage{enumerate}
% \usepackage{rotating}
%\usepackage[font=small,labelfont=bf,labelsep=period]{caption}
\usepackage{listings}
\usepackage{todonotes}

\lstloadlanguages{Python}  % to nie ma dla nas znaczenia

\lstset{language=Python,
basicstyle=\ttfamily\small,
commentstyle=\ttfamily\small,
classoffset=0,
keywordstyle=\ttfamily\small,
classoffset=1,
keywordstyle=\ttfamily\small,
classoffset=0,
stringstyle=\ttfamily\small,
commentstyle=\ttfamily\itshape\small,
numbers=none,
numberstyle=\ttfamily\small,
identifierstyle=\ttfamily\small,
showstringspaces=false,
morekeywords={
}}

\sloppy
\flushbottom
\setlength{\parindent}{7mm}

%\titlelabel{\thetitle.}
%
%\titleformat{\section}{\bfseries\large}{\filright \thesection . }{0ex}{}
%odstępy: lewy, góra, dół
%\titlespacing{\section}{7mm}{24pt plus 0pt minus 1pt}{12pt plus 0pt minus 1pt}

%\titleformat{\subsection}
%{\bfseries}{\filright \thesubsection.\hspace{7.5mm} }{0ex}{}
%odstępy: lewy, góra, dół
%\titlespacing{\section}{7mm}{24pt plus 0pt minus 1pt}{12pt plus 0pt minus 1pt}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}


%\renewcommand{\figurename}{\bf Fig.}
%\renewcommand{\refname}{REFERENCES}




\begin{document}
\thispagestyle{empty}

\hrule

\vspace{2mm}

\noindent
AUTOMATYKA/AUTOMATICS $\bullet$ 2019 $\bullet$ Vol. ? $\bullet$ No. ?

\vspace{2mm}

\hrule

\vspace{22mm}

Piotr Janus\footnote{AGH University of Science and Technology, Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, Krakow, Poland. e-mail: \{piojanus, tomasz.kryjak, mago\}@agh.edu.pl}, Tomasz Kryjak$^*$, Marek Gorgoń$^*$

\vspace{9mm}


{\bf\Large Tutaj tytuł\\[2mm] \indent cd}

\vspace{12mm}

\noindent
{\small \textbf{Abstract}: Abstract
\vspace{12pt}

\noindent
{\small \textbf{Keywords}: }


\section{Introduction}
\label{sec:introduction}

%ten akapit ok
Segmentacja obiektów pierwszoplanowych jest jednym z kluczowych elementów wielu advanced video surveillance systems (AVSS). Jest używana w systemach detekcji i trackowania obiektów oraz human behaviour analysis. Oprócz tego jest również ważnym elementem takich aplikacji jak abadnoned luggage detection and forbidded zone protection. Systemy mogę być użyte w border control i airports).

The simplest group of foreground object detection algorithms
is based on subtracting subsequent frames from a video
sequence. More advanced approaches involve the so-called
background modelling. For each pixel, a dedicated model is
assigned that describes the background appearance in a given
location. Then, depending on used algorithm, the new pixel
value is compared to the background model and classified
(as foreground, background and sometimes also shadow). The
model is updated to incorporate changes in the scene like slow
or fast light variations and movement of objects belonging to
background (i.e a moved chair).

\textbf{TODO} - dlaczego RGB--D jest lepsze, jeden akapit
%However, these BGS techniques have some fundamental
%limitations because they utilized human perception (i.e.,
%visible light) based color spaces such as the red, green, and
%blue (RGB), the hue, saturation, and value (HSV) and the
%YUV where Y and UV represent luminance and chrominance,
%respectively. Basically, those methods are weak to
%color camouflage situations and also sensitive to illumination
%changes.

%TODO opis co zostało zrobione
W niniejszej pracy zaprezentowano rozszerzone wersje powszechnie wykorzystywanych algorytmów do segmentacji obiektów pierwszoplanowych. Opisane metody zostały dostosowane do przetwarzania danych otrzymanych ze standardowej kamery RGB oraz czujnika RGB-D. Skupiono się na algorytmach Gaussian Mixture Model (GMM) i Pixel-Based-Adaptive-Segmenter (PBAS), oprócz modelu programowego przygotowano także implementację sprzętową w układzie GPU z wykorzystaniem technologii CUDA. Do akwizycji obrazu użyto czujnika \textit{Intel Real--Sense} natomiast platforma sprzętowa to \textit{Nvidia Jetson}.

Struktura niniejszego artykułu jest następująca, w rozdziale \ref{sec:prev_work} przedstawiono wcześniejsze prace związane z wykorzystaniem czujników RGB--D do segmentacji obiektów pierwszoplanowych. Sekcja \ref{} opisuje sposób przekazywania sygnału pochodzącego z czujnika RGB--D do platformy obliczeniowej oraz architekturę do przetwarzania obrazu w technologi CUDA. Sekcja \ref{} zawiera opis implementacji sprzętowej w układzie Nvidia Jetson. W ostatnim rozdziale zamieszczono podsumowanie oraz wskazano dalsze kierunki rozwoju.   

\section{Previous work}
\label{sec:prev_work}

%TODO krócej
Autorzy publikacji \cite{Hoffman_2016} przedstawili inne, bardzo ciekawe i niestandardowe podejście do segmentacji obiektów pierwszoplanowych. 
Zaprezentowany algorytm zakłada wykorzystanie sieci neuronowa CNN. 
Do uczenia sieci użyto, oprócz annotowanych danych uczących, także czujnik RGB--D, czyli urządzenie generujące obraz wraz z mapami głębi. 

Oczywiście istnieje wiele metod segmentacji obiektów pierwszoplanowych, wykorzystujących sieci nieuronowe, w tym przypadku autorzy skupili się na usprawnieniu procesu uczenia. 
Standardowo w tego typu algorytmach, sieć uczona jest na podstawie annotowanych obrazów w przestrzeni RGB. 
Zbiory testowe można podzielić na wiele kategorii, jednak uzyskany w ten sposób dokładność detekcji nie zawsze jest zadowalająca. 
W związku z tym autorzy przedstawili hybrydowy system, w którym równolegle uczone są dwie sieci. 
Pierwsza z nich wykorzystuje standardowy zbiór obrazów zapisanych w przestrzeni RGB, natomiast w drugiej sieci wykorzystywana jest mapa głębi tego obrazu. W zaproponowanym podejściu kluczową rolę odgrywa wymiana informacji pomiędzy obiema sieciami CNN w trakcie procesu uczenia. 
Dzięki takiemu rozwiązaniu można dużo efektywniej przeprowadzić taki proces dla obu sieci i następie połączyć je w jedną. 

W pracy \cite{Hasnat_2014} został zaprezentowany kolejny, bazujący na obrazie z czujnika RGB--D, algorytm segmentacji obiektów pierwszoplanowych. W tym przypadku zaproponowano metodę działającą bez nadzoru, przedstawiony algorytm składa się z mechanizmu grupowania w dziedzinie barw i przestrzeni oraz statystycznego łączenia obszarów. Autorzy niestety nie przedstawili implementacji sprzętowej, przetestowano jedynie model programowy.

Wykorzystany algorytm grupowania JCSA (ang. \textit{Joint Color-Spatial-Axial clustering}) służy do estymacji parametrów modelu tła, grupowania pikseli i w efekcie wyodrębnienia regionów na obrazie. Sam model tła jest hybrydowy i składa się z rozkładów Gaussa oraz Watsona. Do wspomnianego wcześniej grupowania pikseli użyto algorytmu BSC (\textit{Bregman Soft Clustering}). W ostatniej fazie metody, czyli łączeniu poszczególnych regionów wykorzystano natomiast graf sąsiedztwa (ang. \textit{RAG -- Region Adjacency Graph}), przedstawiony proces polega na łączeniu odpowiednich wierzchołków w grafie.

Autorzy porównali zaprezentowany algorytm z innymi metodami bazującymi na obrazie głębi. Przeprowadzono testy dla różnych zestawów parametrów i zaproponowano odpowiednie wskaźniki jakości. Wykonane eksperymenty pozwoliły dobrać parametry algorytmu, natomiast uzyskane wyniki potwierdziły wyraźnie większą dokładność w stosunku do zaprezentowanych wcześniej rozwiązań.

Publikacja \cite{Mattoccia_2015} przedstawia czujnik RGB--D w całości zrealizowany w układzie FPGA. System działa w czasie rzeczywistym z częstotliwością powyżej $30$Hz. Wykorzystano do tego celu układ FPGA Spartan 6, do akwizycji obrazu użyto natomiast czujników Aptina pracujących w rozdzielczości $800x480$ pikseli z częstotliwością $60$Hz. Komunikacja z sensorami odbywa się za pośrednictwem magistrali szeregowej $I^2C$.

W pierwszym kroku przeprowadzana jest operacja rektyfikacji obrazu, otrzymywany sygnał pochodzi z dwóch sensorów konieczne jest zatem znalezienie odpowiadających sobie punktów na obu obrazach. Następnym etapem jest zastosowanie transformaty Censusa i operacja dopasowania stereo. Ostatni krok to złożona filtracja obrazu wyjściowego. Na wyjście systemu przekazywany jest 16 bitowy obraz głębi, który następnie jest przesyłany do hosta za pośrednictwem portu USB. Warto zaznaczyć, że autorzy wykorzystali technikę generacji kodu HLS (ang. \textit{High--Level Synthesis}), dzięki temu większość funkcjonalności została zaimplementowana w języku C i~automatycznie przekonwertowana na VHDL.


Autorzy publikacji \cite{Boghdady_2015} zaproponowali wykorzystanie układu GPU do akceleracji sprzętowej algorytmu wykorzystującego rozkłady Gaussa. Przedstawiony system służy do przetwarzania obrazu z kilku kamer jednocześnie. Uzyskane rezultaty nie były jednak zadowalające -- uzyskano przyspieszenie jedynie około 50 procent w stosunku do modelu programowego. W tym przypadku największym ograniczeniem była konieczność transferu dużej ilości danych (kilka obrazów) pomiędzy CPU a GPU. Warto zwrócić uwagę, że autorzy wykorzystali jeden z tańszych układów graficznych dostępnych na rynku -- GeForce GT 730. Aktualnie zdecydowana większość zintegrowanych GPU dysponuje porównywalną lub większą wydajnością. 

 
Praca \cite{Qin_2015} przedstawia implementację algorytmu ViBE z wykorzystaniem układu GPU.
Zaproponowana metoda dodatkowo wykorzystuje uproszczoną metodę Gabor Wavelets do uzyskiwania informacji o krawędziach obrazu. Na tej podstawie odpowiednie piksele zostają wykorzystywane do aktualizacji modelu tła. 
Autorzy dokonali optymalizacji metody, w taki sposób aby możliwe było pełne wykorzystanie potencjału układu GPU. W tym celu operacje dla poszczególnych pikseli wykonywane są niezależnie i mogą być wykonane równolegle. 
Implementacja została przetestowana na platformie sprzętowej składającej się z procesora Intel Core Quad Q8400 CPU oraz procesora graficznego Nvidia GTX 650Ti. Wykorzystano obraz o rozdzielczości 960x540, uzyskana wydajność wynosi odpowiednio $1.8$ i $26$ klatek na sekundę dla CPU i GPU w najbardziej rozbudowanym wariancie algorytmu.


Publikacja \cite{Song_2016} opisuje złożony system wizyjny, w którym układ GPU został wykorzystany do akceleracji algorytmu służącego do detekcji i indeksacji obiektów pierwszoplanowych. 
Zaproponowany system służy do inteligentnego, automatycznego zarządzania energią w pomieszczeniu. 
Oprócz wspomnianego układu GPU wykorzystano także czujnik temperatury. 
Oba urządzenia przy pomocy protokołu Zigbee komunikują się z inteligentnymi licznikami. 
Na podstawie otrzymanych informacji inteligentne liczniki sterują klimatyzacją, oświetleniem i innymi urządzeniami elektrycznymi. 
Jednym z przykładów użycia może być sytuacja gdy zmniejszy się liczba osób w pomieszczeniu temperatura może zostać obniżona. 
Warto dodać, że całość może być także sterowana przy pomocy smartphona. Autorom udało się zapewnić oszczędność energii na poziomie 20-50 procent. Testy przeprowadzono na karcie graficznej GeForce GT 770 i osiągnięto wydajność na poziomie 34 klatek na sekundę w rozdzielczości $768x576$.


W pracy \cite{Guler_2016} autorzy zaimplementowali kilka algorytmów wizyjnych z wykorzystaniem GPU. 
Wśród zrealizowanych metod znalazły się detekcja ruchu, wykrywanie sabotażu kamery, wykrywanie porzuconego bagażu oraz śledzenie obiektów. Dzięki akceleracji GPU udało się uzyskać niemalże 22-krotne przyspieszenie w stosunku do CPU. Do testów użyto procesora NVIDIA Tesla C2075 z architekturą Keplera. 

Do detekcji ruchu została wykorzystana metoda VSAM, jest to klasyczny algorytm wykorzystujący adaptacyjny model  tła, osobny dla poszczególnych pikseli. Model ten jest aktualizowany wraz z każdą kolejną ramką na podstawie wyniku klasyfikacji. Opisana implementacja została także wykorzystana w metodzie służącej do wykrycia sabotażu kamery. Algorytm ten opiera się na porównaniu obraz wejściowego z obrazem tła i wyznaczeniu ich histogramów. Z kolei w celu detekcji przesunięcia kamery porównywane są obrazy tła z dwóch kolejnych ramek. W przypadku, gdy jeden obraz jest przesunięty względem drugiego o dany wektor, oznacza to, że kamera została przesunięta. Detekcja porzuconego obiektu została zaimplementowana z wykorzystaniem algorytmu GMM do segmentacji tła oraz metody indeksacji. Finalnie analiza wyszczególnionych obszarów pozwala wyszukać obiekty statyczne. Dodatkowo autorzy wykorzystali metodę GMM również w algorytmie śledzenia obiektów.


\section{Proposed algorithms}
%TODO tutaj opisać użyte algorytmy
% GMM z modyfikacjami
% PBAS z modyfikacjami
% PBAS z segmentacją obiektów + modyfikacje 

W trakcie badań zostały zaimplementowane dwa różne algorytmy, wykorzystujące zarówno obraz RGB pochodzący z kamery jak i obraz głębi. Pierwszym z nich jest rozszerzona wersja metody \textit{Gaussian Mixture Models}. Implementacja została przygotowana w oparciu o publikację \cite{}. Drugą metodą jest również zmodyfikowana wersja istniejącego już algorytmu \textit{Pixel Based Adaptive Segmenter} \cite{}. Oba algorytmy są zbliżone pod względem koncepcji modelu tła. Jest on niezależny dla każdego piksela i aktualizowany po przetworzeniu każdej ramki obrazu. W kolejnych podsekcjach przedstawiono szczegółowo koncepcję obu metod. 

\subsection{GMM algorithm with RGBD sensor}
\label{subsec:gmm_rgbd}

Gaussian Mixture Models is one of the most commonly used method for background modelling. 
In this approach each pixel is represented by $k$ Gaussian distributions characterized by three parameters ($\omega$, $\mu$, $\sigma^2$). 

$\omega$ is the normalized weight (range 0--1) of the Gaussian distribution.
$\mu$ is the means vector of each colour component of a~particular pixel. 
In the case of RGB colour space it can be defined as the vector of three numbers ($r_{mean}$, $g_{mean}$, $b_{mean}$).
For the grayscale space it is a single number. 

Finally, $\sigma^2$ is the variance of given Gaussian distribution -- a~single value is used for each colour component.
Usually it is assumed that RGB components are independent, which allows to use 3 values instead of a~covariance matrix.
It should be noticed that a~lot of varying implementations of the GMM algorithm have been proposed so far (cf. \cite{Bouwmans_2008}). 
In this work, a~version based on the open source image processing library OpenCV was implemented. 




The background model is initialized while processing the first frame of the video sequence. 
The same initial weight and variance are assigned to each Gaussian distribution, while the vector of mean values is initialized with pixel values. 
The algorithm itself is build up of several steps. 
Firstly, sorting of Gaussian distributions with respect to weight in descending order is performed. 

Then the current pixel ($x$) is tested against each Gaussian distribution. 
For match estimation the Mahalanobins distance formula is applied:
\begin{equation}
\label{equ:mah_dist}
d(x, \mu) = \sqrt{(x-\mu)\cdot(x-\mu)^T}
\end{equation}




A~pixel is classified as matching the Gaussian if the computed distance is lower than established threshold. 
With respect to Equation (\ref{equ:match_test}) usually the triple value of standard deviation is used.
\begin{equation}
\label{equ:match_test}
d(x, \mu) < 3 \cdot \sigma
\end{equation}


The next step is pixel classification based on match test. 
According to Equation (\ref{equ:b_dist}), first $B$ Gaussian distributions, which weights exceed a~constant threshold $T$ are considered as background, otherwise they represent foreground. The default value of this parameter is 0.9 (the same as in OpenCV implementation).


\begin{equation}
\label{equ:b_dist}
B = arg_b min \Bigg( \sum_{i=0}^{b}\omega_i>T \Bigg)
\end{equation}



The final step is model update. 
The following formulas are applied:
\begin{equation}
\label{equ:w_update}
\omega_{i+1} = \omega_{i} + \alpha(M-\omega_i)
\end{equation}
\begin{equation}
\label{equ:u_update}
\mu_{i+1} = \mu_{i} + M\frac{\alpha}{\omega_i} (x-\mu_i)
\end{equation}
\begin{equation}
\label{equ:s_update}
\sigma_{i+1} = \sigma_{i} + M\frac{\alpha}{\omega_i}\Big( (x-\mu_i) \cdot (x-\mu_i)^T \Big)
\end{equation}
\noindent where $\alpha$ represents the learning speed, while $M$ equals 1 for the first Gaussian distribution that passed the match test, otherwise it is 0. 
Moreover the value of variance is upper constrained. 
In the case of distributions, which do not match to pixel value, only the weight value is updated (decreased). 
If instead none of the Gaussian distributions match the pixel, than new Gaussian is added (the same parameters as in the initialization phase are used). 
The distribution with the lowest weight is replaced by the new one. 
Finally, weights have to be normalized to range 0--1. 
Assuming that the number of Gaussian distributions is $k=3$ and each parameter is 12 bits length, the total model size is 180 bits for each pixel.


\subsection{PBAS algorithm with RGBD sensor}
\label{subsec:pbas_rgbd}

Model tła jest nieco bardziej złożony niż ten przedstawiony w algorytmie \textit{ViBE}. Jego pierwsza część, podobnie jak w poprzedniej metodzie, składa się z $N$ zapamiętanych próbek. W celu uproszczenia dalszego zapisu matematycznego, zdefiniujmy ten zbiór jako $B(x_i)$, gdzie $x_i$ to aktualnie przetwarzany piksel obrazu, całość została opisana równaniem (\ref{equ:pbas_model_1}).

	\begin{equation}
		B(x_i)= \left\{ B_1(x_i), B_2(x_i), \dotsc, B_N(x_i) \right\}
	\label{equ:pbas_model_1}	
	\end{equation}

Test dopasowania, jest również bardzo podobny do tego, który występuje w metodzie \textit{ViBE}, jedyną różnicą jest niezależny dla każdego modelu próg przynależności do modelu $R(x_i)$, całość została przedstawiona równaniem (\ref{equ:pbas_test}).

	\begin{equation}
	    F(x_i) = 
		\begin{dcases}
    		1, & \text{gdy } \sum_{k=0}^{N} \{ d(I(x_i), B_k(x_i)) < R(x_i) \} < \#_{min} \\
    		0, & \text{w pozostałych przypadkach} 
		\end{dcases}
	\label{equ:pbas_test}	
	\end{equation}
\noindent Gdzie $d$ to funkcja odległości pomiędzy próbką z modelu tła, a aktualnym pikselem. 

W przypadku algorytmu w wersji \textit{RGB} każdy kanał przetwarzany jest osobno z wykorzystaniem niezależnego modelu tła. Finalna maska jest alternatywą logiczną wyników z poszczególnych kanałów, oznaczając poszczególne maski jako $F_R$, $F_G$, $F_B$ ostateczną klasyfikację możemy zapisać równaniem~(\ref{equ:pbas_final_mask}).

    \begin{equation}
        F_{RGB} = F_R \lor \lor F_G \lor F_B
    \label{equ:pbas_final_mask}
    \end{equation}


Ponieważ każdy kanał analizowany jest osobno, funkcję odległości pomiędzy próbkami można zapisać bardzo prosto równaniem (\ref{equ:pbas_dist}). Jest to po prostu moduł różnicy.
	\begin{equation}
		d(I(x_i),B_k(x_i)) = | I(x_i) - B_k(x_i) |
	\label{equ:pbas_dist}	
	\end{equation}

Kolejnym krokiem po przeprowadzeniu testu dopasowania i klasyfikacji piksela jest aktualizacja modelu tła. Zastosowano konserwatywne podejście, czyli aktualizowane są tylko piksele sklasyfikowane jako tło. Analogicznie jak w algorytmie \textit{ViBE} decyzja o aktualizacji podejmowana jest losowo. Prawdopodobieństwo jej wykonania wynosi $p = 1/T(x_i)$, gdzie parametr $T(x_i)$ jest dynamicznie aktualizowany i niezależny dla każdego piksela. Sama aktualizacja, polega na nadpisaniu, losowo wybranej próbki $B_k(x_i)$ z modelu aktualną wartością piksela $I(x_i)$. Dodatkowo, wybierany jest losowy piksela z otoczenia $3x3$ i losowo wybrana próbka z modelu mu odpowiadającego, jest nadpisywana wartością tego piksela.

Niezależnie od aktualizacji części modelu zawierającej zapamiętane próbki dokonywana jest zmiana parametrów $R(x_i)$ i $T(x_i)$. W tym celu konieczne jest zdefiniowanie kolejnego elementu modelu tła, który zawiera zbiór minimalnych odległości pomiędzy próbką z modelu a aktualną wartością piksela. Zbiór ten został opisany równaniem (\ref{equ:pbas_model_2}). 
 
	\begin{equation}
		D(x_i)= \left\{ D_1(x_i), D_2(x_i) \dotsc, D_N(x_i) \right\}
	\label{equ:pbas_model_2}	
	\end{equation}

Przedstawiony zbiór $D(x_i)$ aktualizowany jest razem ze zbiorem próbek. Nadpisywany jest jedynie element o indeksie $k$ dla którego dystans pomiędzy próbką i aktualnym pikselem jest najmniejsza, zostało to przedstawione równaniem (\ref{equ:pbas_d_min}).
	
	\begin{equation}
		d_{min}(x_i) = min_k d(I(x_i), B_k(x_i))
	\label{equ:pbas_d_min}	
	\end{equation}

Do aktualizacji progu dopasowania, czyli parametru $R(x_i)$ konieczne jest wyznaczenie tzw. miary dynamiki tła, czyli inaczej wartości średniej ze zbioru $D(x_i)$. Finalny wzór na nową wartość progu przedstawia równanie (\ref{equ:pbas_r_update}). Warto dodać, że przyjęto także dolne ograniczenie wartości parametru, wynoszące $R_{low} = 18$.
    
    \begin{equation}
	    R(x_i) = 
		\begin{dcases}
    		R(x_i)(1-R_{inc/dec}), & \text{jeżeli } R(x_i) > \bar{d}_{min}(x_i)R_{sc} \\
    		R(x_i)(1+R_{inc/dec}) & \text{w przeciwnym razie} 
		\end{dcases}
	\label{equ:pbas_r_update}	
	\end{equation}
Gdzie:
\begin{itemize}
	\item[$R_{inc/dec}$] -- stały współczynnik aktualizacji (domyślnie $0.05$)
	\item[$\bar{d}_{min}(x_i)$] -- wartość średnia zbioru $D(x_i)$
	\item[$R_{sc}$] -- współczynnik skalowania (domyślnie $5$)
\end{itemize}

Ostatni etap to aktualizacja parametru opisującego prawdopodobieństwo dokonania aktualizacji, czyli $T(x_i)$. Nowa wartość zależy od wyniku klasyfikacji piksela i została opisana równaniem (\ref{equ:pbas_t_update}). Przyjęto założenie, że parametr ten posiada także ograniczenie dolne jak i górne wynoszące odpowiednio $T_{low}=2$ i $T_{up}=200$. 

    \begin{equation}
	    T(x_i) = 
		\begin{dcases}
    		T(x_i) + \frac{T_{inc}}{\bar{d}_{min}(x_i)}, & \text{jeżeli } F(x_i)=1 \\
    		T(x_i) - \frac{T_{dec}}{\bar{d}_{min}(x_i)} & \text{w przeciwnym razie} 
		\end{dcases}
	\label{equ:pbas_t_update}	
	\end{equation}


\section{Evaluation}
%TODO ewaluacja algorytmów, 
% nagrane sekwencje testowe,
% opracowanie groundtrue
% zastosowane współczynniki
% porównanie z oryginalnymi implementacjami (wyniki z publikacji artykułach)

\section{Hardware implementation}

%TODO 
%opis komunikacji pomiędzy hostem i GPU + rysunek, 
%standardowy algorytm segmetnacji obiektów (model tła, znajduje się w pamięci współdzielonej), 
%opisać co zrobione w OpenCV, a co w CUDA, problem podziału zadań 
%biblioteka intela

% wykorzystane urządzenia - specyfikacja
% przekonwertowanie do tej samej rozdzielczości
% opis zastosowanych modułów
% w przypadku PBASa z indeksacją można dużo zrobić, np własna indeksacja w CUDA (-> conclusion)
% rysunki 
% implementation result
% wydajność





\section{Conclusion}
%w przypadku PBASa z indeksacją można dużo zrobić, np własna indeksacja w CUDA

%\bibliographystyle{plain}
%\bibliography{cars}



\end{document}
